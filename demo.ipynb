{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yavuz/.conda/envs/trial1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yavuz/.conda/envs/trial1/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['HF_HOME'] = '/home/yavuz/yavuz/.cache/huggingface/'\n",
    "import TruthTorchLM as ttlm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TruthTorchLM?\n",
    "\n",
    "TruthTorchLM is an open-source library that collects various state-of-art hallucination detection methods and offers an interface to use and evaluate them in a user-friendly way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#define a huggingface model or  api-based model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.bfloat16).to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=False)\n",
    "\n",
    "api_model = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Usage: Abstain or not abstain\n",
    "The first important functionality of the TruthTorchLM is to generate a message with a truth value. Truth value indicates whether the model is hallucinating or not. Various methods are available to detect hallucination. These methods are named as **truth methods** in the library. Each truth method can have different algorithmic approaches and different output ranges (truth values). For a given truth method, lower truth value means more likely the model is hallucinating, therefore abstain from the output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/yavuz/.conda/envs/trial1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#define 3 different truth methods, other methods can be found under src/TruthTorchLM/truth_methods\n",
    "lars = ttlm.truth_methods.LARS()#https://arxiv.org/pdf/2406.11278\n",
    "confidence = ttlm.truth_methods.Confidence()#average log probality of the generated message\n",
    "self_detection = ttlm.truth_methods.SelfDetection(number_of_questions=5)#https://arxiv.org/pdf/2310.17918\n",
    "\n",
    "truth_methods = [lars, confidence, self_detection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "#define a chat history\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give short and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f\"What is the capital city of France?\"},]\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for model.generate in Huggingface\n",
    "output_hf_model = ttlm.generate_with_truth_value(model = model, tokenizer = tokenizer, messages = chat, truth_methods = truth_methods, max_new_tokens = 100, temperature = 0.7)\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for litellm.completion in litellm\n",
    "output_api_model = ttlm.generate_with_truth_value(model = api_model, messages = chat, truth_methods = truth_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': ' The capital city of France is Paris.', 'normalized_truth_values': [0.7300473799334672, 0.4973040655079717, 0.5], 'unnormalized_truth_values': [0.994862973690033, -0.010783842472449123, -0.0], 'method_specific_outputs': [{'truth_value': 0.994862973690033, 'generated_text': ' The capital city of France is Paris.</s>', 'normalized_truth_value': 0.7300473799334672}, {'truth_value': -0.010783842472449123, 'generated_text': ' The capital city of France is Paris.</s>', 'normalized_truth_value': 0.4973040655079717}, {'truth_value': -0.0, 'entropy': 0.0, 'consistency': 1.0, 'generated_questions': [\" Sure! Here's a rephrased version of the question:\\n\\nWhat city serves as the seat of government and political power for France?\", ' Of course! Here is a rephrased version of the question:\\n\\nWhat city serves as the political and administrative hub of France?', ' Sure! Here is a rephrased version of the question:\\n\\nWhat is the seat of government located in the country of France?', \" Sure! Here's a rephrased version of the question:\\n\\nWhat is the main urban center and seat of government for France, located in the northern half of the country?\", \" Sure, here's a rephrased version of the question:\\n\\nWhat city serves as the capital of France?\"], 'generated_texts': [' The city that serves as the seat of government and political power for France is Paris.', ' The city that serves as the political and administrative hub of France is Paris.', ' The seat of government in France is Paris.', ' The main urban center and seat of government for France is Paris, located in the northern half of the country.', ' The capital of France is Paris.'], 'clusters': [[' The city that serves as the seat of government and political power for France is Paris.', ' The city that serves as the political and administrative hub of France is Paris.', ' The seat of government in France is Paris.', ' The main urban center and seat of government for France is Paris, located in the northern half of the country.', ' The capital of France is Paris.']], 'normalized_truth_value': 0.5}], 'all_ids': tensor([[    1,     1, 29961, 25580, 29962,   887,   526,   263,  8444, 20255,\n",
      "         29889, 25538,  3273,   322, 18378,  6089, 29889,  1724,   338,   278,\n",
      "          7483,  4272,   310,  3444, 29973,   518, 29914, 25580, 29962, 29871,\n",
      "           450,  7483,  4272,   310,  3444,   338,  3681, 29889,     2]]), 'generated_tokens': tensor([29871,   450,  7483,  4272,   310,  3444,   338,  3681, 29889,     2],\n",
      "       device='cuda:0')}\n",
      "{'generated_text': 'The capital city of France is Paris.', 'normalized_truth_values': [0.7300199033425558, 0.4999632544504927, 0.5], 'unnormalized_truth_values': [0.9947235584259033, -0.00014698219829375, -0.0], 'method_specific_outputs': [{'truth_value': 0.9947235584259033, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.7300199033425558}, {'truth_value': -0.00014698219829375, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.4999632544504927}, {'truth_value': -0.0, 'entropy': 0.0, 'consistency': 1.0, 'generated_questions': ['What is the name of the capital of France?', 'What is the name of the capital of France?', 'What is the name of the capital of France?', 'What city serves as the capital of France?', \"What is the name of France's capital?\"], 'generated_texts': ['The capital of France is Paris.', 'The capital of France is Paris.', 'The capital of France is Paris.', 'Paris.', 'The capital of France is Paris.'], 'clusters': [['The capital of France is Paris.', 'The capital of France is Paris.', 'The capital of France is Paris.', 'Paris.', 'The capital of France is Paris.']], 'normalized_truth_value': 0.5}]}\n"
     ]
    }
   ],
   "source": [
    "#print the output of HF model\n",
    "print(output_hf_model)\n",
    "#print the output of API model\n",
    "print(output_api_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration of the truth methods\n",
    "Truth values for different methods are not comparable. They have different ranges and different meanings. Therefore, it would be better to calibrate the truth values to a common range. This can be done by using the `calibrate_truth_method` function. We can define different calibration functions with various objectives. The default calibration is sigmoid normalization where we subtract the threshold from the truth value and divide by the standard deviation and then apply sigmoid function. The standard deviation and threshold is calculated from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: train fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 107546.26it/s]\n",
      "100%|██████████| 10/10 [02:38<00:00, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated with the following parameters: threshold = 0.6613246202468872 std = 0.24197971376981708\n",
      "Calibrated with the following parameters: threshold = -0.30496641806830665 std = 0.08716437970354843\n",
      "Calibrated with the following parameters: threshold = -0.6730116670092565 std = 0.5121234893961678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#we need a supervised dataset to calibrate the truth methods. We use trivia_qa dataset for this example.\n",
    "#we need a correctness evaluator to evaluate the truth methods. We use model_judge for this example. model_judge looks at the model's output and the ground truth and returns a correctness score.\n",
    "model_judge = ttlm.evaluators.ModelJudge('gpt-4o-mini')\n",
    "calibration_results = ttlm.calibrate_truth_method(dataset = 'trivia_qa', model = model, truth_methods = truth_methods, tokenizer = tokenizer, correctness_evaluator = model_judge, \n",
    "    size_of_data = 10,  return_method_details = True, seed = 0, max_new_tokens = 64, wandb_push_method_details = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the truth methods\n",
    "We can evaluate the truth methods with the `evaluate_truth_method` function. We can define different evaluation metrics including AUROC, AUPRC, AUARC, Accuracy, F1, Precision, Recall, PRR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: test fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 119837.26it/s]\n",
      "100%|██████████| 10/10 [02:05<00:00, 12.56s/it]\n"
     ]
    }
   ],
   "source": [
    "results = ttlm.evaluate_truth_method(dataset = 'trivia_qa', model = model, truth_methods=truth_methods, \n",
    "    eval_metrics = ['auroc', 'prr'], tokenizer = tokenizer, size_of_data = 10, correctness_evaluator = model_judge, \n",
    "    return_method_details = True,  batch_generation = True, wandb_push_method_details = False,\n",
    "    max_new_tokens = 64, do_sample = True, seed = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARS {'auroc': 0.9047619047619049, 'prr': 0.8712251850518855}\n",
      "Confidence {'auroc': 0.9047619047619049, 'prr': 0.8712251850518855}\n",
      "SelfDetection {'auroc': 0.8571428571428571, 'prr': 0.7597854413467863}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results['eval_list'])):\n",
    "    print(results['output_dict']['truth_methods'][i],results['eval_list'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Form Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
