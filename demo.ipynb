{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TruthTorchLM as ttlm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'your_openai_key'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TruthTorchLM?\n",
    "\n",
    "TruthTorchLM is an open-source library that collects various state-of-art hallucination detection methods and offers an interface to use and evaluate them in a user-friendly way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#define a huggingface model or  api-based model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.bfloat16).to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=False)\n",
    "\n",
    "api_model = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Usage: Abstain or not abstain\n",
    "The first important functionality of the TruthTorchLM is to generate a message with a truth value. Truth value indicates whether the model is hallucinating or not. Various methods are available to detect hallucination. These methods are named as **truth methods** in the library. Each truth method can have different algorithmic approaches and different output ranges (truth values). For a given truth method, lower truth value means more likely the model is hallucinating, therefore abstain from the output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/yavuz/.conda/envs/trial1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#define 3 different truth methods, other methods can be found under src/TruthTorchLM/truth_methods\n",
    "lars = ttlm.truth_methods.LARS()#https://arxiv.org/pdf/2406.11278\n",
    "confidence = ttlm.truth_methods.Confidence()#average log probality of the generated message\n",
    "self_detection = ttlm.truth_methods.SelfDetection(number_of_questions=5)#https://arxiv.org/pdf/2310.17918\n",
    "\n",
    "truth_methods = [lars, confidence, self_detection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "#define a chat history\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give short and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f\"What is the capital city of France?\"},]\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for model.generate in Huggingface\n",
    "output_hf_model = ttlm.generate_with_truth_value(model = model, tokenizer = tokenizer, messages = chat, truth_methods = truth_methods, max_new_tokens = 100, temperature = 0.7)\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for litellm.completion in litellm\n",
    "output_api_model = ttlm.generate_with_truth_value(model = api_model, messages = chat, truth_methods = truth_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': ' The capital city of France is Paris.', 'normalized_truth_values': [0.7300473799334672, 0.4973040655079717, 0.5], 'unnormalized_truth_values': [0.994862973690033, -0.010783842472449123, -0.0], 'method_specific_outputs': [{'truth_value': 0.994862973690033, 'generated_text': ' The capital city of France is Paris.</s>', 'normalized_truth_value': 0.7300473799334672}, {'truth_value': -0.010783842472449123, 'generated_text': ' The capital city of France is Paris.</s>', 'normalized_truth_value': 0.4973040655079717}, {'truth_value': -0.0, 'entropy': 0.0, 'consistency': 1.0, 'generated_questions': [\" Sure! Here's a rephrased version of the question:\\n\\nWhat city serves as the seat of government and political power for France?\", ' Of course! Here is a rephrased version of the question:\\n\\nWhat city serves as the political and administrative hub of France?', ' Sure! Here is a rephrased version of the question:\\n\\nWhat is the seat of government located in the country of France?', \" Sure! Here's a rephrased version of the question:\\n\\nWhat is the main urban center and seat of government for France, located in the northern half of the country?\", \" Sure, here's a rephrased version of the question:\\n\\nWhat city serves as the capital of France?\"], 'generated_texts': [' The city that serves as the seat of government and political power for France is Paris.', ' The city that serves as the political and administrative hub of France is Paris.', ' The seat of government in France is Paris.', ' The main urban center and seat of government for France is Paris, located in the northern half of the country.', ' The capital of France is Paris.'], 'clusters': [[' The city that serves as the seat of government and political power for France is Paris.', ' The city that serves as the political and administrative hub of France is Paris.', ' The seat of government in France is Paris.', ' The main urban center and seat of government for France is Paris, located in the northern half of the country.', ' The capital of France is Paris.']], 'normalized_truth_value': 0.5}], 'all_ids': tensor([[    1,     1, 29961, 25580, 29962,   887,   526,   263,  8444, 20255,\n",
      "         29889, 25538,  3273,   322, 18378,  6089, 29889,  1724,   338,   278,\n",
      "          7483,  4272,   310,  3444, 29973,   518, 29914, 25580, 29962, 29871,\n",
      "           450,  7483,  4272,   310,  3444,   338,  3681, 29889,     2]]), 'generated_tokens': tensor([29871,   450,  7483,  4272,   310,  3444,   338,  3681, 29889,     2],\n",
      "       device='cuda:0')}\n",
      "{'generated_text': 'The capital city of France is Paris.', 'normalized_truth_values': [0.7300199033425558, 0.4999632544504927, 0.5], 'unnormalized_truth_values': [0.9947235584259033, -0.00014698219829375, -0.0], 'method_specific_outputs': [{'truth_value': 0.9947235584259033, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.7300199033425558}, {'truth_value': -0.00014698219829375, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.4999632544504927}, {'truth_value': -0.0, 'entropy': 0.0, 'consistency': 1.0, 'generated_questions': ['What is the name of the capital of France?', 'What is the name of the capital of France?', 'What is the name of the capital of France?', 'What city serves as the capital of France?', \"What is the name of France's capital?\"], 'generated_texts': ['The capital of France is Paris.', 'The capital of France is Paris.', 'The capital of France is Paris.', 'Paris.', 'The capital of France is Paris.'], 'clusters': [['The capital of France is Paris.', 'The capital of France is Paris.', 'The capital of France is Paris.', 'Paris.', 'The capital of France is Paris.']], 'normalized_truth_value': 0.5}]}\n"
     ]
    }
   ],
   "source": [
    "#print the output of HF model\n",
    "print(output_hf_model)\n",
    "#print the output of API model\n",
    "print(output_api_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration of the truth methods\n",
    "Truth values for different methods are not comparable. They have different ranges and different meanings. Therefore, it would be better to calibrate the truth values to a common range. This can be done by using the `calibrate_truth_method` function. We can define different calibration functions with various objectives. The default calibration is sigmoid normalization where we subtract the threshold from the truth value and divide by the standard deviation and then apply sigmoid function. The standard deviation and threshold is calculated from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: train fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 107546.26it/s]\n",
      "100%|██████████| 10/10 [02:38<00:00, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated with the following parameters: threshold = 0.6613246202468872 std = 0.24197971376981708\n",
      "Calibrated with the following parameters: threshold = -0.30496641806830665 std = 0.08716437970354843\n",
      "Calibrated with the following parameters: threshold = -0.6730116670092565 std = 0.5121234893961678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#we need a supervised dataset to calibrate the truth methods. We use trivia_qa dataset for this example.\n",
    "#we need a correctness evaluator to evaluate the truth methods. We use model_judge for this example. model_judge looks at the model's output and the ground truth and returns a correctness score.\n",
    "model_judge = ttlm.evaluators.ModelJudge('gpt-4o-mini')\n",
    "for truth_method in truth_methods:\n",
    "    truth_method.set_normalizer(ttlm.normalizers.IsotonicRegression())\n",
    "calibration_results = ttlm.calibrate_truth_method(dataset = 'trivia_qa', model = model, truth_methods = truth_methods, tokenizer = tokenizer, correctness_evaluator = model_judge, \n",
    "    size_of_data = 10,  return_method_details = True, seed = 0, max_new_tokens = 64, wandb_push_method_details = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the truth methods\n",
    "We can evaluate the truth methods with the `evaluate_truth_method` function. We can define different evaluation metrics including AUROC, AUPRC, AUARC, Accuracy, F1, Precision, Recall, PRR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: test fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 119837.26it/s]\n",
      "100%|██████████| 10/10 [02:05<00:00, 12.56s/it]\n"
     ]
    }
   ],
   "source": [
    "results = ttlm.evaluate_truth_method(dataset = 'trivia_qa', model = model, truth_methods=truth_methods, \n",
    "    eval_metrics = ['auroc', 'prr'], tokenizer = tokenizer, size_of_data = 10, correctness_evaluator = model_judge, \n",
    "    return_method_details = True,  batch_generation = True, wandb_push_method_details = False,\n",
    "    max_new_tokens = 64, do_sample = True, seed = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARS {'auroc': 0.9047619047619049, 'prr': 0.8712251850518855}\n",
      "Confidence {'auroc': 0.9047619047619049, 'prr': 0.8712251850518855}\n",
      "SelfDetection {'auroc': 0.8571428571428571, 'prr': 0.7597854413467863}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results['eval_list'])):\n",
    "    print(results['output_dict']['truth_methods'][i],results['eval_list'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Form Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning a single score for the long text is not practical and useful. Therefore, we first decompose the generated text into short, single-sentenfe statements. The goal is to assign truth values to these statements.\n",
    "\n",
    "Most truth methods are not directly applicable to assign a truth value to a single statement. To overcome this, TruthTorchLM provides several statement check approaches, which takes turth methods as parameter. Statement check methods are the way we make truth methods usable for decomposed statements. \n",
    "\n",
    "At the end, `long_form_generation_with_truth_value` function returns the generated text, decomposed statements, and the truth values assigned to the statements (as well as all details during the process).\n",
    "\n",
    "Long form generation functionalities of TruthTorchLM is collected under `long_form_generation` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TruthTorchLM.long_form_generation as LFG\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decomposition method that breaks the the long text into statements\n",
    "decomposition_method = LFG.decomposition_methods.StructuredDecompositionAPI(model=\"gpt-4o-mini\", decomposition_depth=1, instruction=ttlm.LFG_DECOMPOSITION_PROMPT) #Utilize API models to decompose text\n",
    "# decomposition_method = LFG.decomposition_methods.StructuredDecompositionLocal(model, tokenizer, decomposition_depth=1, chat_template=DECOMPOSITION_PROMT) #Utilize HF models to decompose text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#entailment model is used by some truth methods and statement check methods\n",
    "model_for_entailment = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-large-mnli').to('cuda:0')\n",
    "tokenizer_for_entailment = DebertaTokenizer.from_pretrained('microsoft/deberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define truth methods \n",
    "confidence = ttlm.truth_methods.Confidence() #average log probality of the generated message\n",
    "lars = ttlm.truth_methods.LARS() #https://arxiv.org/pdf/2406.11278\n",
    "\n",
    "#define the statement check methods that applies truth methods\n",
    "qa_generation = LFG.statement_check_methods.QuestionAnswerGeneration(model=\"gpt-4o-mini\", tokenizer=None, num_questions=2, max_answer_trials=2,\n",
    "                                                                     truth_methods=[confidence, lars], seed=0,\n",
    "                                                                     instruction=ttlm.LFG_QUESTION_GENERATION_PROMPT, \n",
    "                                                                     first_statement_instruction=ttlm.LFG_QUESTION_GENERATION_PROMPT,\n",
    "                                                                     entailment_model=model_for_entailment, entailment_tokenizer=tokenizer_for_entailment) #HF model and tokenizer can also be used, LM is used to generate question\n",
    "#there are some statement check methods that are directly designed for this purpose, not utilizing truth methods\n",
    "as_entailment = LFG.statement_check_methods.AnswerStatementEntailment( model=\"gpt-4o-mini\", tokenizer=None, \n",
    "                                                                      num_questions=3, num_answers_per_question=2, \n",
    "                                                                      instruction=ttlm.LFG_QUESTION_GENERATION_PROMPT, \n",
    "                                                                      first_statement_instruction=ttlm.LFG_QUESTION_GENERATION_PROMPT,\n",
    "                                                                      entailment_model=model_for_entailment, entailment_tokenizer=tokenizer_for_entailment) #HF model and tokenizer can also be used, LM is used to generate question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a chat history\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give brief and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f'Who is Ryan Reynolds?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying stement check method  QuestionAnswerGeneration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying stement check method  AnswerStatementEntailment\n"
     ]
    }
   ],
   "source": [
    "#generate a message with a truth value, it's a wrapper fucntion for model.generate in Huggingface\n",
    "output_hf_model = LFG.long_form_generation_with_truth_value(model=model, tokenizer=tokenizer, messages=chat, fact_decomp_method=decomposition_method, \n",
    "                                          stmt_check_methods=[qa_generation, as_entailment], generation_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "  Ryan Reynolds is a Canadian actor and producer. He is known for his charming wit and good looks, and has appeared in a wide range of films, including romantic comedies, action movies, and superhero films. Some of his most notable roles include:\n",
      "\n",
      "* Green Lantern in the DC Extended Universe\n",
      "* Deadpool in the X-Men franchise\n",
      "* Michael Bryce in the action comedy \"The Hitman's Bodyguard\"\n",
      "* Hal Jordan in the 2011 film \"Green Lantern\"\n",
      "* Andrew Paxton in the romantic comedy \"The Proposal\"\n",
      "\n",
      "Ryan Reynolds has received several awards and nominations for his performances, including a Golden Globe Award and a People's Choice Award. He is married to actress Blake Lively and the couple has two daughters together.\n",
      "\n",
      "Statements:\n",
      "Ryan Reynolds is a Canadian actor.\n",
      "     Conf:  -0.0654496248273034    LARS:  0.9798025488853455    AS Ent:  1.0\n",
      "Ryan Reynolds is a Canadian producer.\n",
      "     Conf:  0.0    LARS:  0.0    AS Ent:  0.0\n",
      "Ryan Reynolds is known for his charming wit.\n",
      "     Conf:  -0.0701493864380535    LARS:  0.38800710439682007    AS Ent:  1.0\n",
      "Ryan Reynolds is known for his good looks.\n",
      "     Conf:  0.0    LARS:  0.0    AS Ent:  0.3333333333333333\n",
      "Ryan Reynolds has appeared in a wide range of films.\n",
      "     Conf:  -0.0902391898497157    LARS:  0.5558112263679504    AS Ent:  1.0\n",
      "Ryan Reynolds has appeared in romantic comedies.\n",
      "     Conf:  -0.07783279622325634    LARS:  0.44845518469810486    AS Ent:  0.5\n",
      "Ryan Reynolds has appeared in action movies.\n",
      "     Conf:  -0.10706259442129573    LARS:  0.9339500963687897    AS Ent:  0.75\n",
      "Ryan Reynolds has appeared in superhero films.\n",
      "     Conf:  0.0    LARS:  0.0    AS Ent:  0.3333333333333333\n",
      "Green Lantern is a character in the DC Extended Universe.\n",
      "     Conf:  -0.04306301346062851    LARS:  0.4952031075954437    AS Ent:  0.0\n",
      "Deadpool is a character in the X-Men franchise.\n",
      "     Conf:  -0.032017308187492974    LARS:  0.9886619448661804    AS Ent:  1.0\n",
      "Michael Bryce is a character in the action comedy \"The Hitman's Bodyguard.\"\n",
      "     Conf:  -0.06931461841424776    LARS:  0.49522900581359863    AS Ent:  1.0\n",
      "Hal Jordan appears in the 2011 film 'Green Lantern'.\n",
      "     Conf:  -0.06366543234751175    LARS:  0.9883058667182922    AS Ent:  1.0\n",
      "Andrew Paxton is a character in the romantic comedy 'The Proposal'.\n",
      "     Conf:  -0.06899468948728847    LARS:  0.9860986471176147    AS Ent:  1.0\n",
      "Ryan Reynolds has received several awards and nominations for his performances.\n",
      "     Conf:  -0.07392175481729851    LARS:  0.6704325675964355    AS Ent:  1.0\n",
      "Ryan Reynolds has received a Golden Globe Award.\n",
      "     Conf:  -0.08815825039446656    LARS:  0.6226838380098343    AS Ent:  1.0\n",
      "Ryan Reynolds has received a People's Choice Award.\n",
      "     Conf:  -0.08561304997497168    LARS:  0.496358186006546    AS Ent:  1.0\n",
      "Ryan Reynolds is married to actress Blake Lively.\n",
      "     Conf:  -0.030518985727050783    LARS:  0.9666363000869751    AS Ent:  1.0\n",
      "Ryan Reynolds and Blake Lively have two daughters together.\n",
      "     Conf:  -0.06318916699849184    LARS:  0.7581892907619476    AS Ent:  0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text:\\n\", output_hf_model['generated_text'])\n",
    "\n",
    "print(\"\\nStatements:\")\n",
    "for i in range(len(output_hf_model['statements'])):\n",
    "    print(output_hf_model['statements'][i]) \n",
    "    print(\"     Conf: \", output_hf_model['unnormalized_truth_values'][0][i][0], \n",
    "          \"   LARS: \", output_hf_model['unnormalized_truth_values'][0][i][1],\n",
    "          \"   AS Ent: \", output_hf_model['unnormalized_truth_values'][1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying stement check method  QuestionAnswerGeneration\n",
      "Applying stement check method  AnswerStatementEntailment\n"
     ]
    }
   ],
   "source": [
    "#generate a message with a truth value, it's a wrapper fucntion for litellm.completion in litellm\n",
    "output_api_model = LFG.long_form_generation_with_truth_value(model=\"gpt-4o-mini\", messages=chat, fact_decomp_method=decomposition_method, \n",
    "                                          stmt_check_methods=[qa_generation, as_entailment], generation_seed=0, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Ryan Reynolds is a Canadian actor, producer, and entrepreneur, known for his roles in films such as \"Deadpool,\" \"The Proposal,\" and \"Free Guy.\" He is also recognized for his comedic talent and charismatic personality. In addition to his film career, Reynolds has been involved in various business ventures, including his ownership of the gin brand Aviation American Gin.\n",
      "\n",
      "Statements:\n",
      "Ryan Reynolds is a Canadian actor.\n",
      "     Conf:  -0.0006410555669399999    LARS:  0.9929046034812927    AS Ent:  1.0\n",
      "Ryan Reynolds is a producer.\n",
      "     Conf:  -0.2420390216741639    LARS:  0.7057953476905823    AS Ent:  1.0\n",
      "Ryan Reynolds is an entrepreneur.\n",
      "     Conf:  0.0    LARS:  0.0    AS Ent:  0.6666666666666666\n",
      "Ryan Reynolds is known for his roles in the film \"Deadpool.\"\n",
      "     Conf:  -0.23288472235094299    LARS:  0.9801365733146667    AS Ent:  1.0\n",
      "Ryan Reynolds is known for his roles in the film \"The Proposal.\"\n",
      "     Conf:  -0.20384988830168754    LARS:  0.41287338733673096    AS Ent:  0.0\n",
      "Ryan Reynolds is known for his roles in the film \"Free Guy.\"\n",
      "     Conf:  0.0    LARS:  0.0    AS Ent:  0.3333333333333333\n",
      "Ryan Reynolds is recognized for his comedic talent.\n",
      "     Conf:  -0.19244640666578827    LARS:  0.9493076205253601    AS Ent:  0.8333333333333334\n",
      "Ryan Reynolds is recognized for his charismatic personality.\n",
      "     Conf:  -0.34624684373718784    LARS:  0.8388969004154205    AS Ent:  0.25\n",
      "Ryan Reynolds has been involved in various business ventures.\n",
      "     Conf:  -40.42633727395585    LARS:  0.7214566171169281    AS Ent:  1.0\n",
      "Ryan Reynolds owns the gin brand Aviation American Gin.\n",
      "     Conf:  -0.25106691279304094    LARS:  0.8857454359531403    AS Ent:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text:\\n\", output_api_model['generated_text'])\n",
    "\n",
    "print(\"\\nStatements:\")\n",
    "for i in range(len(output_api_model['statements'])):\n",
    "    print(output_api_model['statements'][i]) \n",
    "    print(\"     Conf: \", output_api_model['unnormalized_truth_values'][0][i][0], \n",
    "          \"   LARS: \", output_api_model['unnormalized_truth_values'][0][i][1],\n",
    "          \"   AS Ent: \", output_api_model['unnormalized_truth_values'][1][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Long Form Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate truth methods on long form generation by using `evaluate_truth_method_long_form` function. To obtain the correctness labels of the statements we follow SAFE from https://arxiv.org/pdf/2403.18802. SAFE performs google search for each statement and assigns labels as supported, unsupported or irrelevant. We can define different evaluation metrics including AUROC, AUPRC, AUARC, Accuracy, F1, Precision, Recall, PRR. \n",
    "\n",
    "Note: Calibrating truth methods before running evaluation is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAFE utilized serper\n",
    "os.environ['SERPER_API_KEY'] = 'your_serper_api_key'#https://serper.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create safe object that assigns labels to the statements\n",
    "safe = LFG.ClaimEvaluator(rater='gpt-4o-mini', tokenizer = None, max_steps = 2, max_retries = 2, num_searches = 2)\n",
    "\n",
    "#Define metrics\n",
    "sample_level_eval_metrics = ['f1'] #calculate metric over the statements of a question, then average across all the questions\n",
    "dataset_level_eval_metrics = ['auroc', 'prr'] #calculate the metric across all statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... Size of data: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying stement check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [09:32<19:05, 572.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 322.5224087238312\n",
      "Decomposing the generated text...\n",
      "Applying stement check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [12:47<05:50, 350.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 124.50002813339233\n",
      "Decomposing the generated text...\n",
      "Applying stement check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [20:04<00:00, 401.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 241.48697590827942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = LFG.evaluate_truth_method_long_form(dataset='longfact_objects', model='gpt-4o-mini', tokenizer=None,\n",
    "                                sample_level_eval_metrics=sample_level_eval_metrics, dataset_level_eval_metrics=dataset_level_eval_metrics,\n",
    "                                fact_decomp_method=decomposition_method, stmt_check_methods=[qa_generation],\n",
    "                                claim_evaluator = safe, size_of_data=3,  previous_context=[{'role': 'system', 'content': 'You are a helpful assistant. Give precise answers.'}], \n",
    "                                user_prompt=\"Question: {question_context}\", seed=41,  return_method_details = False, return_calim_eval_details=False, wandb_run = None,  \n",
    "                                add_generation_prompt = True, continue_final_message = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stmt_check_methods_0_truth_method_0': {'auroc': 0.477751756440281,\n",
       "  'prr': -0.05373714329351997},\n",
       " 'stmt_check_methods_0_truth_method_1': {'auroc': 0.5386416861826697,\n",
       "  'prr': -0.02268101641869807}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stmt_check_methods_0_truth_method_0 : qa_generation + confidence\n",
    "# stmt_check_methods_0_truth_method_1 : qa_generation + LARS\n",
    "results['dataset_level_eval_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stmt_check_methods_0_truth_method_0': {'f1': {'values': [0.0, 0.0, 0.0],\n",
       "   'mean': 0.0,\n",
       "   'max': 0.0,\n",
       "   'min': 0.0,\n",
       "   'std': 0.0}},\n",
       " 'stmt_check_methods_0_truth_method_1': {'f1': {'values': [0.6363636363636364,\n",
       "    0.88,\n",
       "    0.8292682926829268],\n",
       "   'mean': 0.7818773096821877,\n",
       "   'max': 0.88,\n",
       "   'min': 0.6363636363636364,\n",
       "   'std': 0.10495744653213784}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stmt_check_methods_0_truth_method_0 : qa_generation + confidence\n",
    "# stmt_check_methods_0_truth_method_1 : qa_generation + LARS\n",
    "results['sample_level_eval_list']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfdbdd5cfa929c13cdd22838ac3d22bb2090173664bc1c46031108eafb613df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
