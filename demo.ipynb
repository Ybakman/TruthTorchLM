{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TruthTorchLM as ttlm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'your_openai_key'\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TruthTorchLM?\n",
    "\n",
    "TruthTorchLM is an open-source library that collects various state-of-art truth methods and offers an interface to use and evaluate them in a user-friendly way. \n",
    "\n",
    "TruthTorchLM is compatible with Huggingface and LiteLLM, enabling users to integrate truthfulness assessment into their workflows with minimal code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#define a huggingface model or api-based model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16).to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", use_fast=False)\n",
    "\n",
    "api_model = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Usage: Assessing Truthfulness to Short Generations\n",
    "The first important functionality of the TruthTorchLM is to generate a message with a truth value. Truth value indicates the truthfulness of the generated output. TruthTorchLM offers a wide range of truth methods to assess the truthfulness of the generated output, called **truth methods**. Each truth method can have different algorithmic approaches and different output ranges (truth values). Since the different truth methods have different output ranges, we can not directly compare the truth values of different truth methods. However, for a given truth method, higher truth value means more likely the output is truthful. To make the truth values comparable, we will normalize the truth values to a common range in the next section (Calibrating Truth Methods). Note that normalized truth value in the output dictionary is meaningless without calibration.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define 3 different truth methods, other methods can be found under src/TruthTorchLM/truth_methods\n",
    "lars = ttlm.truth_methods.LARS(device='cuda:0')#https://arxiv.org/pdf/2406.11278\n",
    "confidence = ttlm.truth_methods.Confidence()#average log probality of the generated message\n",
    "\n",
    "truth_methods = [lars, confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "#define a chat history\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give short and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f\"What is the capital city of France?\"},]\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for model.generate in Huggingface\n",
    "output_hf_model = ttlm.generate_with_truth_value(model = model, tokenizer = tokenizer, messages = chat, truth_methods = truth_methods, max_new_tokens = 100, temperature = 0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#generate a message with a truth value, it's a wrapper fucntion for litellm.completion in litellm\n",
    "output_api_model = ttlm.generate_with_truth_value(model = api_model, messages = chat, truth_methods = truth_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'The capital city of France is Paris.', 'normalized_truth_values': [0.7299441133258416, 0.4969400872781441], 'unnormalized_truth_values': [0.9943390488624573, -0.012239803691733818], 'method_specific_outputs': [{'truth_value': 0.9943390488624573, 'generated_text': 'The capital city of France is Paris.<|eot_id|>', 'normalized_truth_value': 0.7299441133258416}, {'truth_value': -0.012239803691733818, 'generated_text': 'The capital city of France is Paris.<|eot_id|>', 'normalized_truth_value': 0.4969400872781441}], 'all_ids': tensor([[128000, 128000, 128006,    882, 128007,    271,   2675,    527,    264,\n",
      "          11190,  18328,     13,  21335,   2875,    323,  24473,  11503,     13,\n",
      "           3639,    374,    279,   6864,   3363,    315,   9822,     30, 128009,\n",
      "         128006,  78191, 128007,    271,    791,   6864,   3363,    315,   9822,\n",
      "            374,  12366,     13, 128009]]), 'generated_tokens': tensor([   791,   6864,   3363,    315,   9822,    374,  12366,     13, 128009],\n",
      "       device='cuda:6')}\n",
      "{'generated_text': 'The capital city of France is Paris.', 'normalized_truth_values': [0.7300072510661978, 0.4999631575975151], 'unnormalized_truth_values': [0.9946593642234802, -0.00014736961020625], 'method_specific_outputs': [{'truth_value': 0.9946593642234802, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.7300072510661978}, {'truth_value': -0.00014736961020625, 'generated_text': 'The capital city of France is Paris.', 'normalized_truth_value': 0.4999631575975151}]}\n"
     ]
    }
   ],
   "source": [
    "#print the output of HF model\n",
    "print(output_hf_model)\n",
    "#print the output of API model\n",
    "print(output_api_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrating Truth Methods\n",
    "Truth values for different methods are not comparable. They have different ranges and different meanings. Therefore, it would be better to calibrate the truth values to a common range. This can be done by using the `calibrate_truth_method` function. We can define different calibration functions with various objectives. The default calibration is sigmoid normalization where we subtract the threshold from the truth value and divide by the standard deviation and then apply sigmoid function. The standard deviation and threshold is calculated from the data. In this example, we use Isotonic Regression for calibration, which calibrates the truth values to [0,1] range and makes the truth values interpretable. With this calibration, we expect 0.8 normalized truth value means that the output is 80% likely to be truthful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: train fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 156503.88it/s]\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated with the following parameters: {'increasing': True, 'out_of_bounds': 'clip', 'y_max': 1.0, 'y_min': 0.0}\n",
      "Calibrated with the following parameters: {'increasing': True, 'out_of_bounds': 'clip', 'y_max': 1.0, 'y_min': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#we need a supervised dataset to calibrate the truth methods. We use trivia_qa dataset for this example.\n",
    "#we need a correctness evaluator to evaluate the truth methods. We use model_judge for this example. model_judge looks at the model's output and the ground truth and returns a correctness score.\n",
    "model_judge = ttlm.evaluators.ModelJudge('gpt-4o-mini')\n",
    "for truth_method in truth_methods:\n",
    "    truth_method.set_normalizer(ttlm.normalizers.IsotonicRegression())\n",
    "calibration_results = ttlm.calibrate_truth_method(dataset = 'trivia_qa', model = model, truth_methods = truth_methods, tokenizer = tokenizer, correctness_evaluator = model_judge, \n",
    "    size_of_data = 10,  return_method_details = True, seed = 0, max_new_tokens = 64, wandb_push_method_details = False, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Truth Methods\n",
    "We can evaluate the truth methods with the `evaluate_truth_method` function. We can define different evaluation metrics including AUROC, AUPRC, AUARC, Accuracy, F1, Precision, Recall, PRR. TruthTorchLM offers a wide range of datasets to evaluate the truth methods. In this example, we use trivia_qa dataset. Note that calibration is suggested for the threshold-based metrics, such as F1, Recall, Precision, and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Huggingface Datasets, split: test fraction of data: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 144631.17it/s]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "results = ttlm.evaluate_truth_method(dataset = 'trivia_qa', model = model, truth_methods=truth_methods, \n",
    "    eval_metrics = ['auroc', 'prr'], tokenizer = tokenizer, size_of_data = 10, correctness_evaluator = model_judge, \n",
    "    return_method_details = True,  batch_generation = True, wandb_push_method_details = False,\n",
    "    max_new_tokens = 64, do_sample = True, seed = 0, pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARS {'auroc': 1.0, 'prr': 1.0}\n",
      "Confidence {'auroc': 0.8888888888888888, 'prr': 0.8762188574381626}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results['eval_list'])):\n",
    "    print(results['output_dict']['truth_methods'][i],results['eval_list'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Truthfulness in Long-Form Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning a single truth value for a long text is neither practical nor useful. TruthTorchLM first decomposes the generated text into short, single-sentence claims and assigns truth values to these claims using claim check methods.\n",
    "\n",
    "\n",
    "Most truth methods are not directly applicable to assign a truth value to a single claim. To overcome this, TruthTorchLM provides several claim check approaches, which takes turth methods as parameter. Claim check methods are the way we make truth methods usable for decomposed claims. Note that there can be some claim check methods that are directly designed for this purpose, not utilizing truth methods.\n",
    "\n",
    "\n",
    "At the end, `long_form_generation_with_truth_value` function returns the generated text, decomposed claims, and the truth values assigned to the claims (as well as all details during the process).\n",
    "\n",
    "\n",
    "Long form generation functionalities of TruthTorchLM is collected under `long_form_generation` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TruthTorchLM.long_form_generation as LFG\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decomposition method that breaks the the long text into claims\n",
    "decomposition_method = LFG.decomposition_methods.StructuredDecompositionAPI(model=\"gpt-4o-mini\", decomposition_depth=2, split_by_paragraphs=False) #Utilize API models to decompose text\n",
    "# decomposition_method = LFG.decomposition_methods.StructuredDecompositionLocal(model, tokenizer, decomposition_depth=1) #Utilize HF models to decompose text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#entailment model is used by some truth methods and claim check methods\n",
    "model_for_entailment = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-large-mnli').to('cuda:0')\n",
    "tokenizer_for_entailment = DebertaTokenizer.from_pretrained('microsoft/deberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define truth methods \n",
    "confidence = ttlm.truth_methods.Confidence() #average log probality of the generated message\n",
    "lars = ttlm.truth_methods.LARS(device='cuda:0') #https://arxiv.org/pdf/2406.11278\n",
    "\n",
    "#define the claim check methods that applies truth methods\n",
    "qa_generation = LFG.claim_check_methods.QuestionAnswerGeneration(model=\"gpt-4o-mini\", tokenizer=None, num_questions=2, max_answer_trials=2,\n",
    "                                                                     truth_methods=[confidence, lars], seed=0,\n",
    "                                                                     entailment_model=model_for_entailment, entailment_tokenizer=tokenizer_for_entailment) #HF model and tokenizer can also be used, LM is used to generate question\n",
    "#there are some claim check methods that are directly designed for this purpose, not utilizing truth methods\n",
    "ac_entailment = LFG.claim_check_methods.AnswerClaimEntailment( model=\"gpt-4o-mini\", tokenizer=None, \n",
    "                                                                      num_questions=3, num_answers_per_question=2, \n",
    "                                                                      entailment_model=model_for_entailment, entailment_tokenizer=tokenizer_for_entailment) #HF model and tokenizer can also be used, LM is used to generate question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a chat history\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give brief and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f'Give some information about Eiffel Tower.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying claim check method  QuestionAnswerGeneration\n",
      "Applying claim check method  AnswerClaimEntailment\n"
     ]
    }
   ],
   "source": [
    "#generate a message with a truth value, it's a wrapper fucntion for model.generate in Huggingface\n",
    "output_hf_model = LFG.long_form_generation_with_truth_value(model=model, tokenizer=tokenizer, messages=chat, decomp_method=decomposition_method, \n",
    "                                          claim_check_methods=[qa_generation, ac_entailment], generation_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " I'd be happy to help!\n",
      "\n",
      "Here's some brief information about the Eiffel Tower:\n",
      "\n",
      "* Location: Paris, France\n",
      "* Height: 324 meters (1,063 feet)\n",
      "* Built: 1889 for the World's Fair\n",
      "* Architect: Gustave Eiffel\n",
      "* Materials: Iron\n",
      "* Original purpose: Radio broadcasting tower\n",
      "* Now: Iconic tourist attraction and symbol of Paris\n",
      "\n",
      "Claims:\n",
      "The Eiffel Tower is located in Paris, France.\n",
      "     Conf: -0.01    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower has a height of 324 meters.\n",
      "     Conf: -0.02    LARS: 0.99    AS Ent: 1.00\n",
      "The height of the Eiffel Tower is 1,063 feet.\n",
      "     Conf: -0.02    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower was built in 1889.\n",
      "     Conf: -0.13    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower was built for the World's Fair.\n",
      "     Conf: -0.06    LARS: 0.99    AS Ent: 1.00\n",
      "The architect of the Eiffel Tower is Gustave Eiffel.\n",
      "     Conf: -0.03    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower is made of iron.\n",
      "     Conf: -0.11    LARS: 0.92    AS Ent: 1.00\n",
      "The original purpose of the Eiffel Tower was as a radio broadcasting tower.\n",
      "     Conf: -10000000000.00    LARS: -10000000000.00    AS Ent: 0.00\n",
      "The Eiffel Tower is now an iconic tourist attraction.\n",
      "     Conf: -0.24    LARS: 0.80    AS Ent: 1.00\n",
      "The Eiffel Tower is now a symbol of Paris.\n",
      "     Conf: -0.25    LARS: 0.97    AS Ent: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text:\\n\", output_hf_model['generated_text'])\n",
    "\n",
    "print(\"\\nClaims:\")\n",
    "for i in range(len(output_hf_model['claims'])):\n",
    "    print(output_hf_model['claims'][i]) \n",
    "    print(f\"     Conf: {output_hf_model['unnormalized_truth_values'][0][i][0]:.2f}\", \n",
    "          f\"   LARS: {output_hf_model['unnormalized_truth_values'][0][i][1]:.2f}\",\n",
    "          f\"   AS Ent: {output_hf_model['unnormalized_truth_values'][1][i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying claim check method  QuestionAnswerGeneration\n",
      "Applying claim check method  AnswerClaimEntailment\n"
     ]
    }
   ],
   "source": [
    "#generate a message with a truth value, it's a wrapper fucntion for litellm.completion in litellm\n",
    "output_api_model = LFG.long_form_generation_with_truth_value(model=\"gpt-4o-mini\", messages=chat, decomp_method=decomposition_method, \n",
    "                                          claim_check_methods=[qa_generation, ac_entailment], generation_seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " The Eiffel Tower is a wrought-iron lattice tower located in Paris, France. It was designed by engineer Gustave Eiffel and completed in 1889 for the Exposition Universelle (World's Fair) held to celebrate the 100th anniversary of the French Revolution. The tower stands approximately 1,083 feet (330 meters) tall, making it one of the tallest structures in the world at the time of its completion.\n",
      "\n",
      "It has three levels accessible to visitors, with restaurants on the first and second levels, and an observation deck on the third level that offers panoramic views of Paris. The Eiffel Tower is an iconic symbol of France and attracts millions of tourists annually. It was initially criticized by some artists and intellectuals but has since become one of the most recognizable landmarks globally.\n",
      "\n",
      "Claims:\n",
      "The Eiffel Tower is a wrought-iron lattice tower located in Paris, France.\n",
      "     Conf: -0.08    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower was designed by engineer Gustave Eiffel.\n",
      "     Conf: -0.16    LARS: 0.97    AS Ent: 1.00\n",
      "The Eiffel Tower was completed in 1889.\n",
      "     Conf: -0.00    LARS: 0.99    AS Ent: 1.00\n",
      "The Eiffel Tower was completed for the Exposition Universelle.\n",
      "     Conf: -0.17    LARS: 0.98    AS Ent: 1.00\n",
      "The Exposition Universelle was held to celebrate the 100th anniversary of the French Revolution.\n",
      "     Conf: -0.12    LARS: 0.96    AS Ent: 0.67\n",
      "The Eiffel Tower stands approximately 1,083 feet tall.\n",
      "     Conf: -0.03    LARS: 0.94    AS Ent: 1.00\n",
      "The Eiffel Tower stands approximately 330 meters tall.\n",
      "     Conf: -0.03    LARS: 0.96    AS Ent: 1.00\n",
      "The Eiffel Tower was one of the tallest structures in the world at the time of its completion.\n",
      "     Conf: -0.31    LARS: 0.97    AS Ent: 0.00\n",
      "The Eiffel Tower has three levels accessible to visitors.\n",
      "     Conf: -0.07    LARS: 0.98    AS Ent: 1.00\n",
      "The first level of the Eiffel Tower has a restaurant.\n",
      "     Conf: -0.16    LARS: 0.97    AS Ent: 1.00\n",
      "The second level of the Eiffel Tower has a restaurant.\n",
      "     Conf: -0.31    LARS: 0.90    AS Ent: 1.00\n",
      "The third level of the Eiffel Tower has an observation deck.\n",
      "     Conf: -0.25    LARS: 0.88    AS Ent: 1.00\n",
      "The observation deck offers panoramic views of Paris.\n",
      "     Conf: -0.25    LARS: 0.92    AS Ent: 1.00\n",
      "The Eiffel Tower is an iconic symbol of France.\n",
      "     Conf: -0.42    LARS: 0.86    AS Ent: 1.00\n",
      "The Eiffel Tower attracts millions of tourists annually.\n",
      "     Conf: -0.23    LARS: 0.60    AS Ent: 1.00\n",
      "The Eiffel Tower was initially criticized by some artists.\n",
      "     Conf: -0.52    LARS: 0.92    AS Ent: 1.00\n",
      "The Eiffel Tower was initially criticized by some intellectuals.\n",
      "     Conf: -141.35    LARS: 0.88    AS Ent: 1.00\n",
      "The Eiffel Tower has become one of the most recognizable landmarks globally.\n",
      "     Conf: -0.45    LARS: 0.95    AS Ent: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text:\\n\", output_api_model['generated_text'])\n",
    "\n",
    "print(\"\\nClaims:\")\n",
    "for i in range(len(output_api_model['claims'])):\n",
    "    print(output_api_model['claims'][i]) \n",
    "    print(f\"     Conf: {output_api_model['unnormalized_truth_values'][0][i][0]:.2f}\", \n",
    "          f\"   LARS: {output_api_model['unnormalized_truth_values'][0][i][1]:.2f}\",\n",
    "          f\"   AS Ent: {output_api_model['unnormalized_truth_values'][1][i]:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Truth Methods in Long-Form Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate truth methods on long form generation by using `evaluate_truth_method_long_form` function. To obtain the correctness of the claims we follow SAFE from https://arxiv.org/pdf/2403.18802. SAFE performs Google search for each claim and assigns labels as supported, unsupported or irrelevant. TruthTorhLM offers different evaluation metrics including AUROC, AUPRC, AUARC, Accuracy, F1, Precision, Recall, PRR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAFE utilized serper\n",
    "os.environ['SERPER_API_KEY'] = 'your_serper_api_key'#https://serper.dev/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create safe object that assigns labels to the claims\n",
    "#for faster run, you can decrease these parameters, but these are the default params in the original SAFE implementation\n",
    "safe = LFG.ClaimEvaluator(rater='gpt-4o-mini', tokenizer = None, max_steps = 5, max_retries = 10, num_searches = 3) \n",
    "\n",
    "#Define metrics\n",
    "sample_level_eval_metrics = ['f1'] #calculate metric over the claims of a question, then average across all the questions\n",
    "dataset_level_eval_metrics = ['auroc', 'prr'] #calculate the metric across all claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... Size of data: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing the generated text...\n",
      "Applying claim check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [04:03<08:07, 243.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 176.2192018032074\n",
      "Decomposing the generated text...\n",
      "Applying claim check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [09:10<04:40, 280.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 247.78795313835144\n",
      "Decomposing the generated text...\n",
      "Applying claim check method  QuestionAnswerGeneration\n",
      "Checking for claim support by google search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [14:18<00:00, 286.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ellapsed for google search: 227.9723973274231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = LFG.evaluate_truth_method_long_form(dataset='longfact_objects', model='gpt-4o-mini', tokenizer=None,\n",
    "                                sample_level_eval_metrics=sample_level_eval_metrics, dataset_level_eval_metrics=dataset_level_eval_metrics,\n",
    "                                decomp_method=decomposition_method, claim_check_methods=[qa_generation],\n",
    "                                claim_evaluator = safe, size_of_data=3,  previous_context=[{'role': 'system', 'content': 'You are a helpful assistant. Give brief and precise answers.'}], \n",
    "                                user_prompt=\"Question: {question_context}\", seed=0,  return_method_details = False, return_calim_eval_details=False, wandb_run = None,  \n",
    "                                add_generation_prompt = True, continue_final_message = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim_check_methods_0_truth_method_0': {'auroc': 0.6754385964912282,\n",
       "  'prr': 0.039265960161460244},\n",
       " 'claim_check_methods_0_truth_method_1': {'auroc': 0.7719298245614037,\n",
       "  'prr': 0.6202041235870827}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stmt_check_methods_0_truth_method_0 : qa_generation + confidence\n",
    "# stmt_check_methods_0_truth_method_1 : qa_generation + LARS\n",
    "results['dataset_level_eval_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim_check_methods_0_truth_method_0': {'f1': {'values': [0.0, 0.0, 0.0],\n",
       "   'mean': 0.0,\n",
       "   'max': 0.0,\n",
       "   'min': 0.0,\n",
       "   'std': 0.0}},\n",
       " 'claim_check_methods_0_truth_method_1': {'f1': {'values': [1.0,\n",
       "    1.0,\n",
       "    0.9230769230769231],\n",
       "   'mean': 0.9743589743589745,\n",
       "   'max': 1.0,\n",
       "   'min': 0.9230769230769231,\n",
       "   'std': 0.03626188621469472}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# stmt_check_methods_0_truth_method_0 : qa_generation + confidence\n",
    "# stmt_check_methods_0_truth_method_1 : qa_generation + LARS\n",
    "results['sample_level_eval_list']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfdbdd5cfa929c13cdd22838ac3d22bb2090173664bc1c46031108eafb613df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
