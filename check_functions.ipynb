{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yavuz/miniconda3/envs/TruthTorchLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2\"\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "import TruthTorchLLM as ttlm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast, PreTrainedModel\n",
    "#import importlib\n",
    "#importlib.reload(ttlm)\n",
    "#reload librarires\n",
    "import json\n",
    "import ast\n",
    "from typing import Union\n",
    "os.environ['SERPER_API_KEY'] = 'beb77fcb0117273454d365ed0e88125e13324f65'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=2)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "entropy = ttlm.truth_methods.Entropy(ttlm.scoring_methods.LogProbScoring(), number_of_generations=2)\n",
    "google_search_check = ttlm.truth_methods.GoogleSearchCheck()\n",
    "matrix_degree_uncertainty = ttlm.truth_methods.MatrixDegreeUncertainty(number_of_generations=2)\n",
    "eccentricity_uncertainty = ttlm.truth_methods.EccentricityUncertainty(number_of_generations=2)\n",
    "num_semantic_set_uncertainty = ttlm.truth_methods.NumSemanticSetUncertainty(number_of_generations=2)\n",
    "sum_eigen_uncertainty = ttlm.truth_methods.SumEigenUncertainty(number_of_generations=2)\n",
    "\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give short and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Who is the Director of 'Karadedeler Olayi'?\"},]\n",
    "\n",
    "truth_dict = ttlm.completion_with_truth_value(model_name, chat, truth_methods = [semantic_entropy,confidence, entropy, google_search_check, matrix_degree_uncertainty, eccentricity_uncertainty, num_semantic_set_uncertainty, sum_eigen_uncertainty], generation_seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Generated Text: {truth_dict[\"generated_text\"]}')\n",
    "print(f\"Unnormalized truth Scores: {truth_dict['unnormalized_truth_values']}\")\n",
    "print(f\"Normalized truth Scores: {truth_dict['normalized_truth_values']}\")\n",
    "print(f\"Method Specific Outputs: {truth_dict['method_specific_outputs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=5)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "entropy = ttlm.truth_methods.Entropy(ttlm.scoring_methods.LogProbScoring(), number_of_generations=2)\n",
    "p_true = ttlm.truth_methods.PTrue()\n",
    "google_search_check = ttlm.truth_methods.GoogleSearchCheck()\n",
    "output_similarity_uncertainty = ttlm.truth_methods.OutputSimilarityUncertainty(number_of_generations=5)\n",
    "\n",
    "\n",
    "chat = [{\"role\": \"system\", \"content\": 'You are a helpful assistant. Give short and precise answers.'},\n",
    "        {\"role\": \"user\", \"content\": f\"Who is the director of the movie Karadedeler Olayi?\"}]\n",
    "\n",
    "\n",
    "truth_dict = ttlm.generate_with_truth_value(model, chat, truth_methods = [semantic_entropy,confidence, entropy, google_search_check, matrix_degree_uncertainty, eccentricity_uncertainty, num_semantic_set_uncertainty, sum_eigen_uncertainty], tokenizer = tokenizer, generation_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mybakman1\u001b[0m (\u001b[33myavuz-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vault/yavuz/yavuz/TruthTorchLLM/wandb/run-20240831_140149-hbec7jvh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yavuz-team/truthtorch_llm/runs/hbec7jvh' target=\"_blank\">truthtorch_llm_experiment_46</a></strong> to <a href='https://wandb.ai/yavuz-team/truthtorch_llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yavuz-team/truthtorch_llm' target=\"_blank\">https://wandb.ai/yavuz-team/truthtorch_llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yavuz-team/truthtorch_llm/runs/hbec7jvh' target=\"_blank\">https://wandb.ai/yavuz-team/truthtorch_llm/runs/hbec7jvh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run = 46\n",
    "wandb_run = wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"truthtorch_llm\",\n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"truthtorch_llm_experiment_{run}\",\n",
    "      # Track hyperparameters and run metadata\n",
    "      job_type=\"train\", config={\"seed\": 1}\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 17/17 [01:10<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth method_0 is calibrated with the following parameters: threshold = -0.08143519171771071 std = 0.19159068762695267\n",
      "Truth method_1 is calibrated with the following parameters: threshold = -0.6391638241381999 std = 1.7195015641714315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">truthtorch_llm_experiment_46</strong> at: <a href='https://wandb.ai/yavuz-team/truthtorch_llm/runs/hbec7jvh' target=\"_blank\">https://wandb.ai/yavuz-team/truthtorch_llm/runs/hbec7jvh</a><br/> View project at: <a href='https://wandb.ai/yavuz-team/truthtorch_llm' target=\"_blank\">https://wandb.ai/yavuz-team/truthtorch_llm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240831_140149-hbec7jvh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=2)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "ttlm.calibrate_truth_method(\"trivia_qa\", \"gpt-3.5-turbo\", truth_methods=[semantic_entropy, confidence], precision = 0.8, fraction_of_data = 0.001)\n",
    "wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 17/17 [01:08<00:00,  4.03s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m confidence \u001b[38;5;241m=\u001b[39m ttlm\u001b[38;5;241m.\u001b[39mtruth_methods\u001b[38;5;241m.\u001b[39mConfidence(ttlm\u001b[38;5;241m.\u001b[39mscoring_methods\u001b[38;5;241m.\u001b[39mLogProbScoring())\n\u001b[1;32m      3\u001b[0m ttlm\u001b[38;5;241m.\u001b[39mevaluate_truth_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrivia_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, truth_methods\u001b[38;5;241m=\u001b[39m[semantic_entropy, confidence], tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, fraction_of_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, return_method_details\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, wandb_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwandb_run\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb_run' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=2)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "ttlm.evaluate_truth_method(\"trivia_qa\", \"gpt-3.5-turbo\", truth_methods=[semantic_entropy, confidence], tokenizer = None, fraction_of_data = 0.001, return_method_details=False, wandb_run = wandb_run)\n",
    "wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=2)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "ttlm.calibrate_truth_method(\"trivia_qa\", \"gpt-3.5-turbo\", truth_methods=[semantic_entropy, confidence], precision = 0.8, fraction_of_data = 0.001, wandb_run = wandb_run, return_method_details=True)\n",
    "wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:34<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is calibrated with the following parameters: threshold = -0.04830607124655001 std = 1.3998440150990843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TruthTorchLLM.truth_methods.confidence.Confidence at 0x7efe502a4070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttlm.calibrate_truth_method(\"trivia_qa\", \"gpt-3.5-turbo\", confidence, tokenizer = None, precision = 0.8, fraction_of_data = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:44<00:00,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is calibrated with the following parameters: threshold = -0.0013171710450170812 std = 0.09989472122474274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TruthTorchLLM.truth_methods.semantic_entropy.SemanticEntropy at 0x7efe502a40d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttlm.calibrate_truth_method(\"trivia_qa\", model, semantic_entropy, tokenizer = tokenizer, precision = 0.8, fraction_of_data = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [00:02<00:42,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [00:05<00:39,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:09<00:45,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:13<00:47,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [00:22<01:05,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [00:24<00:48,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7/17 [00:26<00:37,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [00:36<00:51,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9/17 [00:39<00:36,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10/17 [00:44<00:34,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 11/17 [00:47<00:25,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [00:56<00:28,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 13/17 [01:00<00:20,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14/17 [01:05<00:15,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15/17 [01:08<00:08,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 16/17 [01:13<00:04,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885440\n",
      "403958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:17<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auroc couldn't be calculated because there is only one class. Returning 0.5 as auroc.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'return_model_generations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m semantic_entropy \u001b[38;5;241m=\u001b[39m ttlm\u001b[38;5;241m.\u001b[39mtruth_methods\u001b[38;5;241m.\u001b[39mSemanticEntropy(ttlm\u001b[38;5;241m.\u001b[39mscoring_methods\u001b[38;5;241m.\u001b[39mLengthNormalizedScoring(), number_of_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m confidence \u001b[38;5;241m=\u001b[39m ttlm\u001b[38;5;241m.\u001b[39mtruth_methods\u001b[38;5;241m.\u001b[39mConfidence(ttlm\u001b[38;5;241m.\u001b[39mscoring_methods\u001b[38;5;241m.\u001b[39mLogProbScoring())\n\u001b[0;32m----> 3\u001b[0m \u001b[43mttlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_truth_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrivia_qa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth_methods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msemantic_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction_of_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vault/yavuz/yavuz/TruthTorchLLM/src/TruthTorchLLM/evaluators/eval_truth_method.py:105\u001b[0m, in \u001b[0;36mevaluate_truth_method\u001b[0;34m(dataset, model, truth_methods, tokenizer, eval_metrics, correctness_evaluator, fraction_of_data, system_prompt, user_prompt, seed, return_method_details, wandb_run, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     wandb_run\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m : text_table})\n\u001b[1;32m    103\u001b[0m     wandb_run\u001b[38;5;241m.\u001b[39mlog(eval_dict)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mreturn_model_generations\u001b[49m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: eval_dict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_of_the_model\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(correctness) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(correctness), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_generations\u001b[39m\u001b[38;5;124m'\u001b[39m: model_generations}\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'return_model_generations' is not defined"
     ]
    }
   ],
   "source": [
    "semantic_entropy = ttlm.truth_methods.SemanticEntropy(ttlm.scoring_methods.LengthNormalizedScoring(), number_of_generations=2)\n",
    "confidence = ttlm.truth_methods.Confidence(ttlm.scoring_methods.LogProbScoring())\n",
    "ttlm.evaluate_truth_method(\"trivia_qa\", \"gpt-3.5-turbo\", truth_methods=[semantic_entropy, confidence], tokenizer = None, fraction_of_data = 0.001, wandb_run = wandb_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TruthTorchLLM",
   "language": "python",
   "name": "truthtorchllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
